import os, sys
import gym
import gym.spaces
from gym.utils import seeding
import numpy as np
import configparser
from cffi import FFI
sys.path.append('../')
from edges import Edge

class SimEnv(gym.Env):
    metadata = {'render.modes': ['human']}
    def __init__(self):
        super().__init__()
        config = configparser.ConfigParser()
        config.read('config.ini')
        self.num_agents = config.getint('SIMULATION', 'num_agents')
        self.num_edges  = config.getint('SIMULATION', 'num_edges')
        self.obs_step   = config.getint('TRAINING',   'obs_step')
        datadir         = config.get('SIMULATION',    'datadir')


        actions = [[0, 0, 0, 0, 0, 0],
                [0, 0, 0, 0, 0, 1],
                [0, 0, 0, 0, 1, 0],
                [0, 0, 0, 1, 0, 0],
                [0, 0, 1, 0, 0, 0],
                [0, 1, 0, 0, 0, 0],
                [1, 0, 0, 0, 0, 0]]
        self.actions = np.array(actions)

        self.init = True
        self.flag = True

        self.dist  = np.zeros(self.num_edges * self.obs_step)
        self.width = np.zeros(self.num_edges * self.obs_step)
        self.edges = Edge()
        # for e, idx in self.edges.dict_edge.items():
        for e, idx in self.edges.observed_edge.items():
            fr, to = e
            for n in range(self.obs_step):
                self.dist[idx + n * self.num_edges]  = self.edges.dist[fr, to]
                self.width[idx + n * self.num_edges] = self.edges.width[fr, to]

        agentfn    = os.path.dirname(os.path.abspath(__file__)) + "/../%s/agentlist.txt"%datadir
        self.speed = self.get_speed(agentfn)

        self.action_space      = gym.spaces.Discrete(7)
        self.observation_space = gym.spaces.Box(
                low=0,
                high=self.num_agents,
                shape=np.zeros(self.num_edges * self.obs_step).shape
                )
        self.state = None
        assert self.action_space.n == 7
        assert self.observation_space.shape[0] == self.num_edges * self.obs_step

    def step(self, action):
        _action = self._get_action(action)
        self.call_traffic_regulation(_action, self.num_step)
        self.state = self._get_observation(self.cur_time + self.interval)
        travel_time = self._get_travel_time(self.state)
        reward = self._get_reward(self.state)
        self.episode_reward += travel_time
        print("CURRENT", self.cur_time, action, travel_time, reward, self.episode_reward)
        self.num_step += 1
        self.cur_time += self.interval
        done = self.max_step <= self.num_step
        info = {}
        if done:
            info = {
                    "episode": {
                        "r": self.episode_reward
                        }
                    }
        return self.state, reward, done, info # obseration, reward, done, info

    def reset(self):
        config = configparser.ConfigParser()
        config.read('config.ini')
        self.sim_time  = config.getint('SIMULATION', 'sim_time')
        self.interval  = config.getint('SIMULATION', 'interval')
        self.max_step  = int( np.ceil( self.sim_time / self.interval ))
        self.cur_time  = 0
        self.num_step  = 0
        self.state     = np.zeros(self.num_edges * self.obs_step)

        self.episode_reward = 0
        self.flag = True

        self.reset_sim() # set up simulation

        return self.state

    def render(self, mode='human', close=False):
        pass

    def close(self):
        pass

    def seed(self, seed=None):
        self.np_random, seed = seeding.np_random(seed)
        self.file_seed = seed % 8
        self.seed = seed
        print(self.file_seed)
        return [seed]

    def _get_action(self, action):
        return self.actions[action]

    def _get_reward(self, observation):
        # nrofusers
        # prev_obs = observation[self.num_edges * (self.obs_step-2):self.num_edges * (self.obs_step-1)]
        # obs = observation[self.num_edges * (self.obs_step-1):]
        # return (np.sum(prev_obs) - np.sum(obs)) / self.num_agents
        # tmp2
        # if self._get_travel_time(observation) == 0:
        #     if self.flag:
        #         self.flag = False
        #         return 1
        #     return 0
        # else:
        #     return -0.05

        # exp
        # observation = observation[self.num_edges * (self.obs_step-1):]
        # return np.exp(-np.sum(observation)/self.num_agents) * 2 - 1

        # t_open
        if self.T_open[self.num_step] == 0:
            if self._get_travel_time(observation) == 0:
                return 0
            else:
                return -1
        reward = (self.T_open[self.num_step] - self._get_travel_time(observation)) / (self.T_open[self.num_step])
        if reward < 0:
            return max(reward, -1)
        return min(reward, 1)

        # T_mct  = [52.9275, 85.605, 102.9525, 117.57, 133.095, 140.895, 132.2325, 114.93, 94.2, 60.6, 27.1725, 4.425, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        # T_open = [51.3975, 84.1875, 108.8175, 131.4525, 153.5175, 175.68, 189.54, 193.6575, 187.8075, 171.405, 147.015, 123.1125, 100.89, 76.8375, 53.0475, 34.665, 17.67, 4.0725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
        # if T_mct[self.num_step] == 0:
        #     if self._get_travel_time(observation) == 0:
        #         return 0
        #     else:
        #         return -1
        # return  (T_mct[self.num_step] - self._get_travel_time(observation)) / (T_mct[self.num_step])
        # moving speedをRewardとする
        # 最大値を取るか，平均を取るかは要検討
        # 全歩行者数でRewardを決定
        # v_mean = self.speed
        # th     = 1.8 / (v_mean + 0.3)
        # rho    = observation / (self.dist * self.width)
        # rho    = rho[self.num_edges * (self.obs_step-1):]
        # rho_   = np.where(rho == 0, float('-inf'), rho)
        # v_1    = np.where((0 <= rho) & (rho < th), v_mean, 0)
        # v_2    = np.where((th <= rho_) & (rho_ < 6), 1.8 / rho_ - 0.3, 0)
        # v      = v_1 + v_2
        # reward = (v_mean - v) / v_mean
        # return -reward.mean() 

        # T_mean: randomシナリオの平均，T_min: 混雑を無視した旅行時間
        # T_s    = self._get_travel_time(observation)
        # T_mean = 1848.3963749999998
        # T_min  = 249.71330128490598
        # return (T_s - T_mean) / (T_min - T_mean)
        # 混雑の最大値を抑えたい
        # v_max = self.speed
        # th    = 1.8 / (v_max + 0.3)
        # rho   = np.max(observation)
        # if rho >= 6:
        #     return -np.max(observation)
        # elif th <= rho and rho < 6:
        #     return 1.8 * np.max(observation) ** (-1) - 0.3
        # return 1.0
        # 旅行時間の平均
        # return -self._get_travel_time(observation)
        # 過去Nステップの旅行時間の平均
        # return -np.sum(observation) * self.interval / self.num_agents / self.obs_step

    def _get_travel_time(self, observation):
        observation = observation[self.num_edges * (self.obs_step-1):]
        return np.sum(observation) * self.interval / self.num_agents

    def call_traffic_regulation(self, action, t):
        """
        Take action, i.e., open/close gates
        """
        events = self.get_events(action, t)
        for e in events:
            self.lib.setBombDirect(e.encode('ascii'))
            # print(e)
    
    def call_edge_cnt(self, stop_time=0):
        """
        Count the number of agents on the edge
        """
        self.lib.setStop(stop_time)
        self.lib.iterate()
        # ret = np.zeros(len(self.edges.dict_edge))
        # for e, idx in self.edges.dict_edge.items():
        ret   = np.zeros(len(self.edges.observed_edge))
        for e, idx in self.edges.observed_edge.items():
            fr, to     = e
            ret[idx]   = self.lib.cntOnEdge(fr-1, to-1)
        return ret

    def _get_observation(self, stop_time=0):
        obs     = self.state[self.num_edges:]
        cur_obs = self.call_edge_cnt(stop_time)
        return np.append(obs, cur_obs)

    def get_events(self, action, t):
        """
        e.g., return 1000 traffic_regulation 343 767 1
        e.g., return 1000 traffic_regulation 343 767 0
        """
        time = t * self.interval + 1
        goal_edge = [(767,343), (768,347), (769,766), (770,765), (771,319), (772,150)]
        events = []
        for i in range(len(action)):
            fr, to = goal_edge[i]
            out = "%d traffic_regulation %d %d"%(time, to, fr)
            if action[i] == 1:
                out += " 1"
            else:
                out += " 0"
            # print("TRAFFIC_REGULATION", action, out)
            events.append(out)
        return events

    def call_open_event(self):
        print(self.seed)
        config = configparser.ConfigParser()
        config.read('config.ini')
        datadir = "mkUserlist/data/N80000r{:}i0".format(self.file_seed)
        agentfn  = os.path.dirname(os.path.abspath(__file__)) + "/../%s/agentlist.txt"%datadir
        graphfn  = os.path.dirname(os.path.abspath(__file__)) + "/../%s/graph.twd"%datadir
        goalfn   = os.path.dirname(os.path.abspath(__file__)) + "/../%s/goallist.txt"%datadir
        sim_time = config.get('SIMULATION', 'sim_time')
        argv = [sys.argv[0]]
        argv.extend([
            agentfn,
            graphfn,
            goalfn,
            "-o",
            "bin/result%s"%self.seed,
            "-l",
            "9999999",
            "-e",
            sim_time
            ])
        tmp = []
        for a in argv:
            tmp.append(self.ffi.new("char []", a.encode('ascii')))
        argv = self.ffi.new("char *[]", tmp)
        # call simulator
        self.lib.init(len(argv), argv)
        _action = self._get_action(0)
        self.call_traffic_regulation(_action, 0)
        print("self.call_edge_cnt")

        self.T_open = [np.sum(self.call_edge_cnt((i + 1) * self.interval)) * self.interval / self.num_agents for i in range(self.max_step)]
        print(self.T_open, np.sum(self.T_open))

    def set_bomb(self):
        pass

    def reset_sim(self):
        print("reset simulator configure")
        config = configparser.ConfigParser()
        config.read('config.ini')
        # datadirs = config.get('SIMULATION', 'datadirs')
        # datadir  = np.random.choice(datadirs)
        # datadir  = config.get('SIMULATION', 'datadir')
        datadir = "mkUserlist/data/N80000r{:}i0".format(self.file_seed)
        agentfn  = os.path.dirname(os.path.abspath(__file__)) + "/../%s/agentlist.txt"%datadir
        graphfn  = os.path.dirname(os.path.abspath(__file__)) + "/../%s/graph.twd"%datadir
        goalfn   = os.path.dirname(os.path.abspath(__file__)) + "/../%s/goallist.txt"%datadir
        sim_time = config.get('SIMULATION', 'sim_time')
        if self.init:
            libsimfn = os.path.dirname(os.path.abspath(__file__)) + "/../bin/libsim.so"
            self.ffi = FFI()
            self.lib = self.ffi.dlopen(libsimfn)
            self.ffi.cdef("""
            void init(int argc, char** argv);
            int  setStop(int t);
            void iterate();
            void setBombDirect( char *text);
            int  cntOnEdge(int fr, int to);
            void restart();
            void init_restart(int argc, char** argv);
            """) 
            self.call_open_event()
            print("INIT")
            # argv = [sys.argv[0]]
            # argv.extend([
            #     agentfn,
            #     graphfn,
            #     goalfn,
            #     "-o",
            #     "result",
            #     "-l",
            #     "9999999",
            #     "-e",
            #     sim_time
            #     ])
            # tmp = []
            # for a in argv:
            #     tmp.append(self.ffi.new("char []", a.encode('ascii')))
            # argv = self.ffi.new("char *[]", tmp)
            # call simulator
            # self.lib.init(len(argv), argv)
            self.lib.restart()
            # save initial state
            self.init = False
        else:
            # argv = [sys.argv[0]]
            # argv.extend([
            #     agentfn,
            #     graphfn,
            #     goalfn,
            #     "-o",
            #     "result",
            #     "-l",
            #     "9999999",
            #     "-e",
            #     sim_time
            #     ])
            # tmp = []
            # for a in argv:
            #     tmp.append(self.ffi.new("char []", a.encode('ascii')))
            # argv = self.ffi.new("char *[]", tmp)
            # load inital state
            self.lib.restart()


    def get_speed(self, agentfn):
        with open(agentfn) as f:
            lines = f.readlines()
            return sum([float(l.split('\t')[2]) for l in lines[1:]]) / float(lines[0].split(' ')[0])

    def dummy_step(self, action):
        reward = 0
        _action = self._get_action(action)
        self.call_traffic_regulation(_action, self.num_step)
        self.state = self._get_observation(self.cur_time + self.interval)
        travel_time = self._get_travel_time(self.state)
        self.episode_reward += travel_time
        print("CURRENT", self.cur_time, action, travel_time, reward, self.episode_reward)
        self.num_step += 1
        self.cur_time += self.interval
        done = self.max_step <= self.num_step
        info = {}
        if done:
            info = {
                    "episode": {
                        "r": self.episode_reward
                        }
                    }
        return self.state, reward, done, info # obseration, reward, done, info
